<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DLT-META</title><link>https://databrickslabs.github.io/dlt-meta/</link><description>Recent content on DLT-META</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 04 Aug 2021 14:50:11 -0400</lastBuildDate><atom:link href="https://databrickslabs.github.io/dlt-meta/index.xml" rel="self" type="application/rss+xml"/><item><title>Metadata Preparation</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/metadatapreperation/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/metadatapreperation/</guid><description>Directory structure conf/ onboarding.json silver_transformations.json dqe/ bronze_data_quality_expectations.json Create onboarding.json Create silver_transformations.json Create data quality rules json&amp;rsquo;s for each entity e.g. Data Quality Rules The onboarding.json file contains links to silver_transformations.json and data quality expectation files dqe.
onboarding.json File structure: Examples( Autoloader, Eventhub, Kafka ) env is your environment placeholder e.g dev, prod, stag
Field Description data_flow_id This is unique identifier for pipeline data_flow_group This is group identifier for launching multiple pipelines under single DLT source_format Source format e.</description></item><item><title>DLT-META CLI</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/dltmeta_cli/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/dltmeta_cli/</guid><description>pre-requisites: Databricks CLI Python 3.8.0 + Steps: git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta python -m venv .venv source .venv/bin/activate pip install databricks-sdk OnboardJob Run Onboarding using dlt-meta cli command: databricks labs dlt-meta onboard Above command will prompt you to provide onboarding details. If you have cloned dlt-meta git repo then accepting defaults will launch config from demo/conf folder. You can create onboarding files e.g onboarding.json, data quality and silver transformations and put it in conf folder as show in demo/conf Once onboarding jobs is finished deploy bronze and silver DLT using below command Dataflow DLT Pipeline: Deploy Bronze DLT databricks labs dlt-meta deploy Above command will prompt you to provide dlt details.</description></item><item><title>DLT-META Manual</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/dltmeta_manual/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/dltmeta_manual/</guid><description>OnboardJob Option#1: using Databricks Python whl job Go to your Databricks landing page and do one of the following:
In the sidebar, click Jobs Icon Workflows and click Create Job Button.
In the sidebar, click New Icon New and select Job from the menu.
In the task dialog box that appears on the Tasks tab, replace Add a name for your jobâ€¦ with your job name, for example, Python wheel example.</description></item><item><title>DAIS DEMO</title><link>https://databrickslabs.github.io/dlt-meta/demo/dais/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/demo/dais/</guid><description>DAIS 2023 DEMO: DAIS 2023 Session Recording This demo showcases DLT-META&amp;rsquo;s capabilities of creating Bronze and Silver DLT pipelines with initial and incremental mode automatically.
Customer and Transactions feeds for initial load Adds new feeds Product and Stores to existing Bronze and Silver DLT pipelines with metadata changes. Runs Bronze and Silver DLT for incremental load for CDC events Steps to launch DAIS demo in your Databricks workspace: Launch Terminal/Command promt</description></item><item><title>Tech Summit DEMO</title><link>https://databrickslabs.github.io/dlt-meta/demo/techsummit/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/demo/techsummit/</guid><description>Databricks Tech Summit FY2024 DEMO: This demo will launch auto generated tables(100s) inside single bronze and silver DLT pipeline using dlt-meta.
Launch Terminal/Command prompt
Install Databricks CLI
git clone https://github.com/databrickslabs/dlt-meta.git
cd dlt-meta
Set python environment variable into terminal
export PYTHONPATH=&amp;lt;&amp;lt;local dlt-meta path&amp;gt;&amp;gt; Run the command python demo/launch_techsummit_demo.py --username=ravi.gawai@databricks.com --source=cloudfiles --cloud_provider_name=aws --dbr_version=13.3.x-scala2.12 --dbfs_path=dbfs:/techsummit-dlt-meta-demo-automated
cloud_provider_name : aws or azure or gcp</description></item><item><title>Append FLOW Autoloader Demo</title><link>https://databrickslabs.github.io/dlt-meta/demo/append_flow_cf/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/demo/append_flow_cf/</guid><description>Append FLOW Autoloader Demo: This demo will perform following tasks:
Read from different source paths using autoloader and write to same target using dlt.append_flow API Read from different delta tables and write to same silver table using append_flow API Add file_name and file_path to target bronze table for autoloader source using File metadata column Append flow with autoloader Launch Terminal/Command prompt
Install Databricks CLI</description></item><item><title>Append FLOW Eventhub Demo</title><link>https://databrickslabs.github.io/dlt-meta/demo/append_flow_eh/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/demo/append_flow_eh/</guid><description>Append FLOW Autoloader Demo: Read from different eventhub topics and write to same target tables using dlt.append_flow API Steps: Launch Terminal/Command prompt
Install Databricks CLI
git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal
dlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Eventhub
Needs eventhub instance running
Need two eventhub topics first for main feed (eventhub_name) and second for append flow feed (eventhub_name_append_flow)</description></item><item><title>Silver Fanout Demo</title><link>https://databrickslabs.github.io/dlt-meta/demo/silver_fanout/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/demo/silver_fanout/</guid><description>Silver Fanout Demo This demo will perform following steps Showcase onboarding process for silver fanout pattern Run onboarding for the bronze cars table, which contains data from various countries. Run onboarding for the silver tables, which have a where_clause based on the country condition in silver_transformations_cars.json. Run Bronze for cars tables Run onboarding for the silver tables, fanning out from the bronze cars tables to country-specific tables such as cars_usa, cars_uk, cars_germany, and cars_japan.</description></item><item><title>Integration Tests</title><link>https://databrickslabs.github.io/dlt-meta/additionals/integration_tests/</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/additionals/integration_tests/</guid><description>Run Integration Tests Initial steps Prerequisite: Datatbricks CLI installed as given here git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta python -m venv .venv source .venv/bin/activate pip install databricks-sdk dlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run integration test against cloudfile or eventhub or kafka using below options: If databricks profile configured using CLI then pass --profile &amp;lt;profile-name&amp;gt; to below command otherwise provide workspace url and token in command line
2a.</description></item><item><title>Execution</title><link>https://databrickslabs.github.io/dlt-meta/faq/execution/</link><pubDate>Wed, 04 Aug 2021 14:26:55 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/faq/execution/</guid><description>Q. How do I get started ?
Please refer to the Getting Started guide
Q. How do I create metadata DLT-META ?
DLT-META needs following metadata files:
Onboarding File captures input/output metadata Data Quality Rules File captures data quality rules Silver transformation File captures processing logic as sql Q. What is DataflowSpecs?
DLT-META translates input metadata into Delta table as DataflowSpecs
Q. How many DLT pipelines will be launched using DLT-META?</description></item><item><title>General</title><link>https://databrickslabs.github.io/dlt-meta/faq/general/</link><pubDate>Wed, 04 Aug 2021 14:50:11 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/faq/general/</guid><description>Q. What is DLT-META ?
DLT-META is a solution/framework using Databricks Delta Live Tables aka DLT which helps you automate bronze and silver layer pipelines using CI/CD.
Q. What are the benefits of using DLT-META ?
With DLT-META customers needs to only maintain metadata like onboarding.json, data quality rules and silver transformations and framework will take care of execution. In case of any input/output or data quality rules or silver transformation logic changes there will be only metadata changes using onboarding interface and no need to re-deploy pipelines.</description></item></channel></rss>